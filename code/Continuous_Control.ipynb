{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. make environment and check some envrionment's attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.exe', no_graphics=True)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define Actor/Critic and Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.bn1(self.fcs1(state)))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ddpg_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 256  # minibatch size\n",
    "GAMMA = 0.9  # discount factor\n",
    "TAU = 1e-3  # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3  # learning rate of the actor\n",
    "LR_CRITIC = 1e-3  # learning rate of the critic\n",
    "WEIGHT_DECAY = 0  # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones, n_learn):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            for _ in range(n_learn):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.1):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        for sarsd in zip(states, actions, rewards, next_states, dones):\n",
    "            e = self.experience(sarsd[0],sarsd[1],sarsd[2],sarsd[3],sarsd[4])\n",
    "            self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(\n",
    "            device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(\n",
    "            device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. define ddpg() method for train agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, n_episodes=1000, max_t=1000, print_every=100, n_agents=20, learn_period=20, n_learn=10):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        score_batch = np.zeros(n_agents)\n",
    "        agent.reset()\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones, n_learn)\n",
    "            states = next_states\n",
    "            score_batch += rewards\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        mean_score_batch = np.mean(score_batch)\n",
    "        scores_deque.append(mean_score_batch)\n",
    "        scores.append(mean_score_batch)\n",
    "        mean_score_100ep = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score_100ep))\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, mean_score_100ep))\n",
    "        if mean_score_100ep >= 30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, mean_score_100ep))\n",
    "            torch.save(agent.actor_local.state_dict(), '../weight/checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), '../weight/checkpoint_critic.pth')\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. train the agent and plot the result.\n",
    "- agent's weight is saved in `../weight/checkpoint_actor.pth`, `../weight/checkpoint_critic.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrjang\\Miniconda3\\envs\\drlnd-gpu\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.00\n",
      "Episode 2\tAverage Score: 0.16\n",
      "Episode 3\tAverage Score: 0.15\n",
      "Episode 4\tAverage Score: 0.30\n",
      "Episode 5\tAverage Score: 0.48\n",
      "Episode 6\tAverage Score: 0.63\n",
      "Episode 7\tAverage Score: 0.82\n",
      "Episode 8\tAverage Score: 0.99\n",
      "Episode 9\tAverage Score: 1.22\n",
      "Episode 10\tAverage Score: 1.36\n",
      "Episode 11\tAverage Score: 1.61\n",
      "Episode 12\tAverage Score: 1.77\n",
      "Episode 13\tAverage Score: 1.90\n",
      "Episode 14\tAverage Score: 2.09\n",
      "Episode 15\tAverage Score: 2.29\n",
      "Episode 16\tAverage Score: 2.48\n",
      "Episode 17\tAverage Score: 2.70\n",
      "Episode 18\tAverage Score: 2.86\n",
      "Episode 19\tAverage Score: 3.07\n",
      "Episode 20\tAverage Score: 3.29\n",
      "Episode 21\tAverage Score: 3.58\n",
      "Episode 22\tAverage Score: 3.78\n",
      "Episode 23\tAverage Score: 3.99\n",
      "Episode 24\tAverage Score: 4.19\n",
      "Episode 25\tAverage Score: 4.33\n",
      "Episode 26\tAverage Score: 4.45\n",
      "Episode 27\tAverage Score: 4.64\n",
      "Episode 28\tAverage Score: 4.80\n",
      "Episode 29\tAverage Score: 4.91\n",
      "Episode 30\tAverage Score: 5.00\n",
      "Episode 31\tAverage Score: 5.21\n",
      "Episode 32\tAverage Score: 5.39\n",
      "Episode 33\tAverage Score: 5.59\n",
      "Episode 34\tAverage Score: 5.86\n",
      "Episode 35\tAverage Score: 6.25\n",
      "Episode 36\tAverage Score: 6.59\n",
      "Episode 37\tAverage Score: 7.02\n",
      "Episode 38\tAverage Score: 7.52\n",
      "Episode 39\tAverage Score: 8.04\n",
      "Episode 40\tAverage Score: 8.57\n",
      "Episode 41\tAverage Score: 9.07\n",
      "Episode 42\tAverage Score: 9.55\n",
      "Episode 43\tAverage Score: 10.09\n",
      "Episode 44\tAverage Score: 10.52\n",
      "Episode 45\tAverage Score: 10.99\n",
      "Episode 46\tAverage Score: 11.49\n",
      "Episode 47\tAverage Score: 11.90\n",
      "Episode 48\tAverage Score: 12.21\n",
      "Episode 49\tAverage Score: 12.55\n",
      "Episode 50\tAverage Score: 13.01\n",
      "Episode 51\tAverage Score: 13.42\n",
      "Episode 52\tAverage Score: 13.81\n",
      "Episode 53\tAverage Score: 14.18\n",
      "Episode 54\tAverage Score: 14.57\n",
      "Episode 55\tAverage Score: 14.89\n",
      "Episode 56\tAverage Score: 15.19\n",
      "Episode 57\tAverage Score: 15.37\n",
      "Episode 58\tAverage Score: 15.55\n",
      "Episode 59\tAverage Score: 15.81\n",
      "Episode 60\tAverage Score: 16.08\n",
      "Episode 61\tAverage Score: 16.37\n",
      "Episode 62\tAverage Score: 16.61\n",
      "Episode 63\tAverage Score: 16.86\n",
      "Episode 64\tAverage Score: 17.08\n",
      "Episode 65\tAverage Score: 17.32\n",
      "Episode 66\tAverage Score: 17.56\n",
      "Episode 67\tAverage Score: 17.78\n",
      "Episode 68\tAverage Score: 17.98\n",
      "Episode 69\tAverage Score: 18.23\n",
      "Episode 70\tAverage Score: 18.41\n",
      "Episode 71\tAverage Score: 18.61\n",
      "Episode 72\tAverage Score: 18.77\n",
      "Episode 73\tAverage Score: 18.98\n",
      "Episode 74\tAverage Score: 19.17\n",
      "Episode 75\tAverage Score: 19.34\n",
      "Episode 76\tAverage Score: 19.50\n",
      "Episode 77\tAverage Score: 19.67\n",
      "Episode 78\tAverage Score: 19.81\n",
      "Episode 79\tAverage Score: 19.96\n",
      "Episode 80\tAverage Score: 20.14\n",
      "Episode 81\tAverage Score: 20.33\n",
      "Episode 82\tAverage Score: 20.51\n",
      "Episode 83\tAverage Score: 20.65\n",
      "Episode 84\tAverage Score: 20.81\n",
      "Episode 85\tAverage Score: 20.99\n",
      "Episode 86\tAverage Score: 21.13\n",
      "Episode 87\tAverage Score: 21.28\n",
      "Episode 88\tAverage Score: 21.44\n",
      "Episode 89\tAverage Score: 21.59\n",
      "Episode 90\tAverage Score: 21.73\n",
      "Episode 91\tAverage Score: 21.84\n",
      "Episode 92\tAverage Score: 21.92\n",
      "Episode 93\tAverage Score: 22.01\n",
      "Episode 94\tAverage Score: 22.13\n",
      "Episode 95\tAverage Score: 22.27\n",
      "Episode 96\tAverage Score: 22.39\n",
      "Episode 97\tAverage Score: 22.47\n",
      "Episode 98\tAverage Score: 22.57\n",
      "Episode 99\tAverage Score: 22.71\n",
      "Episode 100\tAverage Score: 22.81\n",
      "Episode 100\tAverage Score: 22.81\n",
      "Episode 101\tAverage Score: 23.17\n",
      "Episode 102\tAverage Score: 23.51\n",
      "Episode 103\tAverage Score: 23.84\n",
      "Episode 104\tAverage Score: 24.18\n",
      "Episode 105\tAverage Score: 24.54\n",
      "Episode 106\tAverage Score: 24.88\n",
      "Episode 107\tAverage Score: 25.19\n",
      "Episode 108\tAverage Score: 25.51\n",
      "Episode 109\tAverage Score: 25.83\n",
      "Episode 110\tAverage Score: 26.14\n",
      "Episode 111\tAverage Score: 26.45\n",
      "Episode 112\tAverage Score: 26.75\n",
      "Episode 113\tAverage Score: 27.06\n",
      "Episode 114\tAverage Score: 27.34\n",
      "Episode 115\tAverage Score: 27.63\n",
      "Episode 116\tAverage Score: 27.92\n",
      "Episode 117\tAverage Score: 28.21\n",
      "Episode 118\tAverage Score: 28.51\n",
      "Episode 119\tAverage Score: 28.80\n",
      "Episode 120\tAverage Score: 29.09\n",
      "Episode 121\tAverage Score: 29.33\n",
      "Episode 122\tAverage Score: 29.55\n",
      "Episode 123\tAverage Score: 29.80\n",
      "Episode 124\tAverage Score: 30.04\n",
      "\n",
      "Environment solved in 124 episodes!\tAverage Score: 30.04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_AGENTS = 20\n",
    "LEARN_PERIOD, N_LEARN = 20, 10\n",
    "\n",
    "agent = Agent(state_size=33, action_size=4, random_seed=0)\n",
    "scores = ddpg(env, n_episodes=2000, n_agents=N_AGENTS, learn_period=LEARN_PERIOD, n_learn=N_LEARN)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
